逻辑回归算法 操作指南

```
逻辑回归（Logistic Regression）是一种分类算法，虽然名字中带有回归。
其实是线性回归基础上的魔性改造；
-- 逻辑回归的应用场景
    广告点击率：点击 或者 点击
    是否为垃圾邮件： 是 或者 不是
    是否患病：是 或者 不是
    金融诈骗：是 或者 不是
    虚假账号：是 或者 不是
属于两个类别之间的判断。
但是也可以通过改造，进行多分类的判断；
逻辑回归就是解决二分类问题的利器。

-- 逻辑回归的原理
1）输入值是什么
2）如何判断逻辑回归的输出

f(x) = w1 x1 + w2 x2 + w3 x3 + …… + b
f(x) = w^T x  ， 值域是实数集 
输入就是一个线性回归的结果。
线性回归的结果是，返回一个确定w 和b 的模型；
当输入测试集，就会产生预测值；

实际需求是，想要预测各个分类的概率，概率在0-1之间，预测 p(y=1)  和 p(y=0) 的值；
设定一个界限0.5， 概率大于0.5,判定y=1;概率小于0.5，判定y=0;

sigmoid函数，做值域的映射；

	g(z) = 1/(1 + e^-z )
	
这个函数，就可以将值域为实数集，映射到0-1；
函数图像显示，当z趋向于负无穷，g(z)趋向于0；当z趋向于正无穷，g(z)趋向于1；当z=0，g(z)=0.5;

所以将线性回归，代入sigmoid函数；
h(w) = w1 x1 + w2 x2 + w3 x3 + …… + b

g(w^T , x) = 1/(1 + e^-h(w) )
          =1/(1 + e^-w^T x)
          
此时，给定测试集x,就得到概率值

1)模型：sigmoid函数 + 线性回归

将线性回归的值域（-inf,+inf）-->(0,1)

2)判断标准
    回归的结果输入到sigmoid函数当中
    输出结果：
    [0, 1]区间中的一个概率值，默认为0.5为阈值
    大于阈值属于true ,小于阈值属于false;
    逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，
    并且这个类别默认标记为1(正例),
    另外的一个类别会标记为0(反例)。

通过模型，对测试集，进行预测；预测的类别结果，跟实际类别结果肯定有差异。

如何去描述损失函数呢？

              -log(p)    y=1
cost(p,y) = 
              -log(1-p)  y=0
              
p(y=1|x) 给定x 的情况下，y=1 的概率越大越好，对应的损失函数越小越好 ；            
p(y=0|x) 给定x 的情况下，y=0 的概率越大越好，即 1 - p(y=1|x)  越大越好，也就是 p(y=1|x)越小越好，对应的损失函数越小越好；

-- 损失以及优化
逻辑回归的损失，称之为对数似然损失

                -log(hθ(x))   if y=1
cost(hθ(w),y) = 
                -log(1-hθ(x)) if y=0
                
其中y 为真实值，hθ(x)

我们可以把分段函数写在一起：

cost(hθ(w),y) = 1/m sum(i:1~m)（ -yi log(hθ(w))  - (1 - yi)log(1-hθ(x))  ）

yi:样本i的真实标签 0 或者 1 ；
当yi=1,后一项没有了；
当yi=0,前一项没有了；
m:样本数；

1）线性回归w 和d 确定，h(w) = w1 x1 + w2 x2 + w3 x3 + …… + b 确定；
	给出测试集，就会产生预测值；
2）经过sigmoid函数映射，
g(w^T , x) = 1/(1 + e^-h(w) )
          =1/(1 + e^-w^T x)
	将预测值映射成概率；

3）再将 真实分类值 和 预测概率值 代入综合损失函数：
cost(hθ(w),y) =1/m  sum(i:1~m)（ -yi log(hθ(w))  - (1 - yi)log(1-hθ(x))  ）

--推导
sigmoid函数变换：
hθ(x) = g(θ^T x) = 1/(1 + e^(θ^T x) )

p(y=1|x,θ) = 1/(1 + e^(θ^T x) ) = hθ(x)
p(y=0|x,θ) = 1 - 1/(1 + e^(θ^T x) ) = 1 - hθ(x)
也可以把上面两个式子，写成一个式子：

p(y=i|x,θ) = hθ(x)^y ( 1 - hθ(x))^(1-y)

运用极大似然函数，即联合概率；测试集的所有样本，都是同时发生；
前提样本独立同分布p(AB) = P(A)P(B), y 只能取0和1
如果不是独立分布，p(AB) = P(A|B)P(B)= P(B|A)P(A)  贝叶斯公式

X --> Y    X:{X1,X2,X3,,,Xm}  Y:{Y1,Y2,Y3,,,Ym}  给定测试集数据；

P(Y|X;θ) = P(y1|x1;θ) P(y2|x2;θ) P(y3|x3;θ) ,,, P(ym|xm;θ)= 连乘 P(yi|xi;θ)  各个样本独立同分布；

p(y=i|x,θ) = hθ(x)^y ( 1 - hθ(x))^(1-y)  sigmoid函数求得各个样本概率值；

于是得到：

P(Y|X;θ) = 连乘 P(yi|xi;θ)     p(y=i|x,θ) = hθ(x)^y ( 1 - hθ(x))^(1-y)   

L(θ) = 连乘 P(yi|xi;θ) = 连乘（ hθ(xi)^yi ( 1 - hθ(xi))^(1-yi) ）

取对数，进行对数变换：

log(ab)= log(a) + log(b)

l(θ)= log(L(θ))= sum(i:1~m)  ( yi log( hθ(xi)) + (1-yi) log(1 - hθ(xi))    )  极大似然函数；

想得到极大似然函数，即联合概率的最大值；
我们一般很少去求最大值，一般去求最小值，做一个单调递减变换；

L（θ）= -1/m l(θ)  （损失函数）

因此，已知

l(θ)= log(L(θ))= sum(i:1~m)  ( yi log( hθ(xi)) + (1-yi) log(1 - hθ(xi))  )

L（θ）= -1/m l(θ)

得到：

J(θ)= -1/m sum(i:1~m)( yi log( hθ(xi)) + (1-yi) log(1 - hθ(xi))  )

上式就是最终的损失函数；也叫对数似然损失，或者交叉熵损失；

分类概率越大越好，损失函数越小越好；

保证了似然函数求最大值，损失函数求最小值；

-- 优化
同样使用梯度下降优化算法，去减少损失函数的值。
这样去更新逻辑回归前面对应算法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率。

1）凸优化（凸函数优化）
有唯一极值的函数，是凸函数；
金森不等式：函数任意两个点（x1,y1）,（x2,y2）,满足f(（x1+x2）/2 ) <= (f(x1)+f(x2))/2
有多个极值的函数，是非凸函数；
凸函数：一定能优化到全局最优解
非凸函数也可以做凸优化（神经网络）结果是局部最优解
对函数求导，凸函数有唯一一个点，导数等于0；非凸函数有多个点，导数等于0；

J(θ)= -1/m sum(i:1~m)( yi log( hθ(xi)) + (1-yi) log(1 - hθ(xi))  )

hθ(x) = g(θ^T x) = 1/(1 + e^(θ^T x) )吗

因为：
g(z) = 1/(1 + e^-z ) 对 z 求导：
g `(z) = g(z)g(1-z)  这就是sigmoid函数的强大之处
所以：

J`(θ)|θ= -1/m sum(i:1~m)( yi 1/hθ(xi) - (1-yi) 1/(1 - hθ(xi))  ) h`θ(xi)
       = -1/m sum(i:1~m)( yi 1/hθ(xi) - (1-yi) 1/(1 - hθ(xi)) ) g(θ^T xi)g(1-θ^T xi) `(θ^T xi)|θ
       = -1/m sum(i:1~m)( yi 1/hθ(xi) - (1-yi) 1/(1 - hθ(xi)) )xi/j
       = -1/m sum(i:1~m)( yi-g(θ^T xi))xi/j
       = 1/m sum(i:1~m)(hθ(x)-yi) xi/j
       
j:特征值，发现跟特征值有关，所以也需要对特征值进行标准化，不然容易受到异常值的影响；

梯度下降算法：

已知：

θj:=θj - αJ`(θ)              J`(θ)|θ=1/m sum(i:1~m)(hθ(x)-yi) xi/j
得到：

θj:=θj - α 1/m sum(i:1~m)(hθ(x)-yi) xi/j

梯度下降，依次进行迭代；
……

最终得到最优解；


--正则化
J(θ)= -1/m  c sum(i:1~m)( yi log( hθ(xi)) + (1-yi) log(1 - hθ(xi))  )  + sum(i:1~m) θi^2
c:惩罚系数，
c 越大，惩罚越轻微，模型走向过拟合；
c 越小，惩罚越厉害，模型走向欠拟合；

-- 逻辑回归api
>> sklearn.linear_model.LogisticRegression(solver='liblinear', penalty=‘l2’, C = 1.0)

    solver可选参数:{'liblinear', 'sag', 'saga','newton-cg', 'lbfgs'}，
    默认: 'liblinear'；用于优化问题的算法。
    对于小数据集来说，“liblinear”是个不错的选择，而“sag”和'saga'对于大型数据集会更快。
    对于多类问题，只有'newton-cg'， 'sag'， 'saga'和'lbfgs'可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。
    penalty：正则化的种类;L1,L2
    C：正则化力度,惩罚项系数
    
    
默认将类别数量少的当做正例

LogisticRegression方法相当于 SGDClassifier(loss="log", penalty=" "),SGDClassifier实现了一个普通的随机梯度下降学习。而使用LogisticRegression(实现了SAG)


-- 分类评估方法(仅仅针对二分类模型)

准确率，在类别不平衡时，不能代表模型真实的水平；
准确率并不是衡量分类模型的最好标准；
在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)

1）混淆矩阵
              预测结果
         正例           负例
正例      TP             FN   

负例      FP             TN

T：预测对了
F: 预测错了
P:预测正例
N:预测负例  
>> from sklearn.metrics import confusion_matrix
>> confusion_matrix(y_test,y_predict)
输出的矩阵刚好是反的

    0   1
0  TN  FP
1  FN  TP

第一行第一列对应的是0

>> tn,fp,fn,tp = confusion_matrix(y_test,y_predict).ravel()
将矩阵拉平，把每个数获取出来；
>> from sklearn.metrics import precision_score,recall_score
y_true,y_pred这两个数组，元素可能不是0,1，需要转化成 0,1

>> precision_score(y_true,y_pred)
得到精确率
>> recall_score(y_true,y_pred)
得到召回率

精确率(Precision)：预测结果为正例样本中，真实为正例的比例
TP/(TP+FP)
召回率(Recall)：真实为正例的样本中，预测结果为正例的比例（查得全，对正样本的区分能力）
TP/(TP+FN)

精确率和召回率是相互抗衡的；

F1-score，反映了模型的稳健型；同时衡量精确率和召回率；
F1-score=2TP/(2TP + FP + FN)=2pre*rec/(pre+rec)
pre,rec同时高，F1-score值才会高；

>> from sklearn.metrics import f1_score
>> f1_score(y_true,y_pred) 



-- 分类评估报告api
>> sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )
	此时，不需要转换成 0,1；
    y_true：真实目标值
    y_pred：估计器预测目标值
    labels:指定类别对应的数字
    target_names：目标类别名称
    return：每个类别精确率与召回率
    
>> from sklearn.metrics import classification_report
>> print(classification_report(y_test,y_predict,labels=(2,4),target_names=['良性','恶性']))
因为精确率和召回率，数组元素2,4分别作为1，就会有两套不同的算法；

上面所有指标，对于样本不平衡的时候，就会失效；

-- TPR与FPR
TPR = TP / (TP + FN)
所有真实类别为1的样本中，预测类别为1的比例
FPR = FP / (FP + TN)
所有真实类别为0的样本中，预测类别为1的比例

-- ROC曲线
ROC曲线的横轴就是FPRate，纵轴就是TPRate，曲线下的面积就是auc，因为FPRate，TPRate，值域都是0~1，auc肯定小于1；
roc在副对角线上面，0.5<auc<1 ;正常情况；
副对角线下的面积是0.5，如果瞎猜roc就是副对角线，auc是副对角线下的面积；
如果roc在副对角线下面，auc小于0.5，比瞎猜还要差；是故意猜错；
我们的目的是auc越大越好，面积接近于1；
所以当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5


-- AUC指标
Area Under roc Curve，就是ROC曲线的积分，也是ROC曲线下面的面积。
AUC只能用来评价二分类
AUC非常适合评价样本不平衡中的分类器性能
意义是随机取一对正负样本，正样本得分大于负样本得分的概率
AUC的范围在[0, 1]之间，并且越接近1越好，越接近0.5属于乱猜
AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
一般的模型auc>=0.8,这种模型是最好的；

-- AUC计算API
>> from sklearn.metrics import roc_auc_score
>> y_score = lr.predict_proba(new_x_test)
算出各个样本属于不同类别的概率
>> y_score = y_score[:,1]
取类别为1的概率的那一列
>> y_true= np.where( y_test >3,1,0)
进行转化，变成0和1的数组
>> roc_auc_score(y_true,y_score)	
得到auc的值
	
计算ROC曲线面积，即AUC值
y_true：每个样本的真实类别，必须为0(反例),1(正例)标记
y_score：预测得分，可以是正类的估计概率、置信值或者分类器方法的返回值

-- ROC曲线的绘制
y_true,y_score已知，y_score降序排列;
不断调整阈值，得到一系列对应的点（fpr,tpr）；
根据点，绘制roc曲线，就得到曲线的面积auc；
表明任取一个正样本和一个负样本，正样本概率大于负样本概率的情况；


-- 逻辑回归解决多分类问题

1）ovr(one vs rest)  m=k(类别数)
拿其中的一个类别作为一类，其他的类别作为一类，训练出一个模型；
再次拿其他类别中的一个，做上述操作；
这样就得到多个模型;
类别：k1,k2,k3,,,
模型：m1,m2,m3,,,
给定测试值，输入到模型，就得到，预测为对应类别的概率值；
选取所有模型对应概率值中，最大的概率值对应的类别；
作为测试值，对应的类别；
每个逻辑回归，也就是每个模型，最终对应一条线，
l1,l2,l3,,,
这些线，相交围成的公共区域，如果区域内出现样本点，就不能判断是属于什么类别，出现不可分类的情况；

2）ovo(one vs one)  m=k*(k-1)/2
每次拿出其中的两个类别，其他的类别不做处理；
训练出一个模型；
再拿出其他组合，都重复上述操作；
得到多个模型；
类别：k1,k2,k3,,,
模型：m1,m2,m3,,,
给定测试值，输入到模型，就得到，预测为对应类别的概率值；
每个类别的概率值对应都会有多个，求概率的平均值；
选取最大的概率值对应的类别，作为测试值对应的类别；

-- softmax回归
将逻辑回归的sigmoid函数换成softmax函数（指数归一化），
softmax函数可以同时将多个值映射成多个概率。
y1 = w1 x1 + w2 x2 + w3 x3 + ,,,
y2 = w1 x1 + w2 x2 + w3 x3 + ,,,
y3 = w1 x1 + w2 x2 + w3 x3 + ,,,
,,,

进行指数变换，不改变单调性；
e^y1
e^y2
e^y3
，，，

值域变为大于0；

进行归一化：
e^y1/(e^y1 + e^y2 + e^y3)
e^y2/(e^y1 + e^y2 + e^y3)
e^y3/(e^y1 + e^y2 + e^y3)
,,,
最终完成映射：
y1-->   e^y1/(e^y1 + e^y2 + e^y3)
y2-->   e^y2/(e^y1 + e^y2 + e^y3)
y3-->   e^y3/(e^y1 + e^y2 + e^y3)
,,,

因为y1，y2，y3，，，之间存在竞争关系，y1，y2，y3，，，值发生改变，最终映射的概率值也不一样；






```

