决策树算法

```
-- 决策树定义：
1）是一种树形结构，
2）本质是一棵由多个判断节点组成的树

两个‘不纯度’指标
	1）信息熵
	2）gini系数（基尼系数）
	
做分类：	
三个‘最优分割’指标
	1）信息增益       ID3    ID3只能对离散属性的数据集构成决策树
	2）信息增益率     C4.5   优化后解决了ID3分支过程中总喜欢偏向选择值较多的 属性
	3）gini信息增益   CART   可以进行分类和回归，可以处理离散属性，也可以处理连续属性
	
做回归：mse 均方差--方差最小化 或者 mae

ID3算法：
	采用信息增益；
	偏向选择属性值较多的特征；
C4.5算法：
    (1)用信息增益率来选择属性
    (2)可以处理连续数值型属性
    (3)采用了一种后剪枝方法
    (4)对于缺失值的处理
CART算法：
	采用gini信息增益；
	强制二叉树；
多变量决策树(multi-variate decision tree)：
	选择多个特征的线性组合来进行分割；
决策树变量的两种类型：
	1）数值型：
		排序；
		相邻点中点作为分割点 < 和 >= 分两叉;
	2）类别型：
		使用“=”来分割，是不是
		
如何评估分割点的好坏？
    1） 指标：
        信息熵
        信息增益率
        gini信息增益
    2） 思想：
        贪心算法：当前利益最大化；
《图解算法》

决策树：
是一种树形结构，
其中每个内部节点表示一个属性上的判断，
每个分支代表一个判断结果的输出，
最后每个叶节点代表一种分类结果，
本质是一棵由多个判断节点组成的树。

目的：

利用数据集的某个特征值的属性进行划分，相对应的目标值也会呈现不同的比例分布，通过一定的公式，计算出一个指标，通过这个指标的大小，进而判断选择哪个特征值划分数据集比较合适；

-- 熵 Entropy 是“混乱”程度的量度
越有序，熵越低;越混乱,熵越高;
熵越大，越不纯；熵越小，就越纯；
系统越有序，熵值越低；系统越混乱或者分散，熵值越高。

"信息熵" (information entropy)是度量 样本集合纯度 最常用的一种指标。

整体熵，Ent(D) 就是只看 目标值 所占比例情况，根据公式计算的 熵；

假定当前样本集合 D 中 第 k 个 目标值 所占样本集合的比例为 P(k),k:(1,2,3,,,),
P(k) = C(k)/D ,
C(k) 为第 k 个 目标值 所占样本集合的数量,
D为样本集合的所有样本数量,

则 D 的 信息熵 定义为(（log是以2为底，lg是以10为底）:

Ent(D) = -sum(i:1~n) C(k)/D * log(C(k)/D)
	   = -sum(i:1~n) P(k) * log(P(k))
	   = -p1 * log(p1) -p2 * log(p2) -,,,pn * log(pn)
	   = -（p1 * logp1 + p2 * logp2 + ... + pn * logpn）
	   
其中：Ent(D) 的值越小，则 D 的纯度越高.
数据越纯，信息熵就越低；越不纯，信息熵就越高；


-- 信息增益  
减熵的过程就是信息增益的过程；
某列特征插入后，信息增益可以最大化；

使用划分前后 集合熵 的 差值 来衡量使用当前特征的属性对于样本集合 D 划分效果的好坏。

假定特征 a 有 V 个可能的属性取值:
    就把 数据集 划分成 v 份，
    可以计算每一份 子集 的 熵，
    再把每一份 子集 的 熵 加权平均，

a(1),a(2),,,a(v)
若使用 a 的 v 个属性 来对样本集 D 进行划分，则会产生 V 个分支结点,
D(v) 代表 D 中 a 特征值的属性 v 的样本数，
样本数越多的分支结点的影响越大, P(v) = D(v)/D
则
Ent(D(v1)= sum(k:1~k)  C(k)/D(v1) * log(C(k)/D(v1))
Ent(D(v2)= sum(k:1~k)  C(k)/D(v2) * log(C(k)/D(v2))
，，，

Ent(D|a)= sum(v:1~v) D(v)/D * Ent(D(v))
        = D(v1)/D * Ent(D(v1)) +  D(v2)/D * Ent(D(v2)) + ，，，
        = sum(v:1~v) D(v)/D * (sum(k:1~k)  C(k)/D(v) * log(C(k)/D(v)))

每个子集的熵 =  每个属性的熵 * 该属性所占子集的比例，再求和

v:分割子集数，属性的个数；
D：数据集的总样本数；
D(v):第 v 个属性子集的样本数 
Ent(D(v))：第 v 个属性子集中，各个目标值所占比例的熵
C(k): 第 k 个 目标值 所占属性子集 的样本数        


减熵 = 信息增益
向系统注入信息，也就可以将系统的熵减少；

特征 a 对训练数据集 D 的 信息增益 Gain(D,a),
定义为集合 D 的 信息熵 Ent(D) 与 给定特征 a 条件下 D 的 信息条件熵 Ent(D|a) 之差，
信息条件熵 Ent(D|a)，就是按照 a 特征的属性划分数据集，计算得出的熵；

信息增益公式为：

整体样本熵Ent(D)：
Ent(D) = - sum(k:1~n) P(k) *log(P(k))

信息条件熵 Ent(D|a)：
Ent(D|a) = sum(v:1~v) D(v)/D * Ent(D(v))

信息增益Gain(D,a)：
Gain(D,a) = Ent(D)- Ent(D|a)
          = Ent(D)- sum(v:1~v) D(v)/D * Ent(D(v))
         
即：
信息增益 = entroy(前) - entroy(后)          


    v:分割子集数，属性的个数；
    D：样本数
    D(v):第 v 个属性子集的样本数
    Ent(D(v))：第 v 个属性子集中，各个目标值所占比例的熵

信息增益 越大，则意味着使用 特征 a 的属性 来进行划分所获得的 "纯度提升" 越大。

直到每个 属性子集 的 各个目标值所占比例 的 熵 为 0 或为 1，不再划分；
否则，不纯的 属性子集 继续按照 其他特征的属性 进行划分，直到 纯 为止；

因此，我们可用 信息增益 来进行 决策树 的 划分属性选择；
 
但是， 
信息增益准则 对 可取 属性值 数目较多 的 特征  有所偏好,
信息增益 会选择，划分属性比较多的特征；属性子集 分的越多，数据就越 纯，每个属性子集各个目标值所占比例的 熵 就会越低，信息增益 就会最大；

但是，容易导致选取一些没有用的特征，因为其属性特别多，划分后各目标值所占比例的熵等于0，进而达不到选取最好特征值进行划分的目的，得不到结果；
 

不过，
使用 "增益率" (gain ratio) 来选择 最优特征 进行划分.

-- 增益率
增益率 是用前面的 信息增益 Gain(D, a) 和 特征 a 对应的 "固有值" (intrinsic value) 的比值来共同定义的。
信息增益率 = 特征a的 信息增益 / 特征a 本身的 熵

    信息增益Gain(D,a)：
    Gain(D,a) = Ent(D) - Ent(D|a)
    信息增益Gain(D,a) = 整体样本熵 Ent(D) - 信息条件熵 Ent(D|a)

用 分裂信息度量 来考虑某种属性进行分裂时 分支的 数量信息 和 尺寸信息，我们把这些信息称为 属性 的 内在信息（instrisic information）；
也可以理解为 熵 的计算，不过不按 目标值所占属性子集比例的熵 进行计算，而是按 属性子集所占数据集比例的熵 计算：

IV(a) = - sum(v:1~v) D(v)/D * log D(v)/D
    D(v)：第v个属性子集的样本数
    D：数据集的总样本数；

Gain_ratio = Gain(D,a)/IV(a)
    Gain(D,a)：信息增益
    IV(a)：分割特征 本身的 熵
    IV(a) 随着特征值得增大而增大，Gain(D,a) 随着特征值得增大而增大，从而相互抵消；

数据集的某特征的 信息增益率 高一些，在构建决策树的时候，该特征 就会被优先选择；
在 选取节点 的过程中，降低了 属性取值较多的特征的 选取偏好。
--- 树的形成

选取信息增益率最高的特征进行划分，会发现该特征的某个属性子集中，
其中一个目标值所占比例1，其他目标值所占比例为0，
则表示该属性子集很纯；
该特征的其他属性子集，根据已经有的属性子集样本，再按照其他特征的属性进行重新划分，
也能选取一个最优特征，也会发现某个属性子集纯了；
，，，
以此类推,
样本数每次都会减少，属性子集样本数每次也会减少，最后肯定会出现，都纯了；
数据集每次都能得到最优特征即子节点，每次都能划出一个属性即叶子节点，随着越来越纯，子节点和叶子节点就都确定了；就形成了树；


while(当前节点"不纯")：
    1.计算当前节点的类别熵(以类别取值计算)
    2.计算当前阶段的属性熵(按照属性取值吓得类别取值计算)
    3.计算信息增益
    4.计算各个属性的分裂信息度量
    5.计算各个属性的信息增益率
end while
当前阶段设置为叶子节点
 

-- gini系数 也是来衡量 不纯度 的：
数据集中的特征值有的是离散型，有的是数值型；
	和 熵 的作用一模一样；但是 gini系数 计算简单；
	熵 是 对数计算 ，在计算机中 对数 是不能直接计算的，是通过 级数 近似计算，计算复杂；
	gini系数 是乘法计算；
	1）不纯度越高，熵越大，基尼系数值越高；
	2）不纯度越低，熵越小，基尼系数值越低；
     


数据集 D 的 纯度 可用 基尼值 来度量: 

-- 基尼值 Gini（D）：
类似于整体熵 ；
Gini(D) = 1 - sum(k:1~k) p(k)^2   
    p(k)= C(k)/D ,
    D 为样本的数量，
    C(k) 为第 k 个目标值所占样本集合的数量
   
-- 基尼指数Gini_index(D,a)：
Gini_index(D,a) = sum(k:1~k) D(k)/D * Gini(D(k))
			   = sum(k:1~k) D(k)/D * p(k)^2
类似于信息条件熵的计算；
    k:分割子集数，属性的个数；
    D：样本数
    D(k):第 k 个属性子集的样本数
    Ent(D(k))：第 k 个属性子集中，各个目标值所占比例的 基尼值

当数据集按照某个特征的属性进行划分，该特征值是离散的，某个属性子集中目标值所占比例，其中一个目标值为1，其他目标值为0，该属性子集纯了，计算得到该属性基尼值为0；

当数据集按照某个特征的属性进行划分，发现属性的个数大于2个，依然要分成2类，就会有很多种分法；假如属性有n个，则有n(n-1)/2种方法；计算该属性子集的基尼指数，就会有n(n-1)/2种；对比计算结果，取Gini指数最小的分组作为划分结果；

当数据集按照某个特征的属性进行划分，该特征值是连续的，需要将该特征的属性进行降序排列；选取相邻属性值的中位数作为分割点，假设有n个属性，则有n-1个分割点，要将所有属性分成两类，有(n-1)个分法；计算每一种分法该属性子集的基尼指数，就会有(n-1)个对应值；对比计算结果，取Gini指数最小的分组作为划分结果；

但是，如果特征值非常多，那么分割点也非常多，计算量就相当大；

为了减少分割点，可以选择目标值不一样的对应的连续值中间，选取相邻的属性值得平均值作为分割点；
分割点减少，对应计算量就下降了；
再去对比计算结果，取Gini指数最小的分组作为划分结果；


-- 基尼增益
gini信息增益 = gini系数(前) - gini系数(后) 

-- 基尼指数，存在信息增益同样的缺点：
	信息增益对可取值数目较多的属性有所偏好；
	采用强制二叉树，就避免了以上问题；

-- 特征处理方式：
	1）二类特征--直接二叉树
	2）多类特征--A,B,C,D
		遍历每一种不同的二分割模式，分别计算gini系数
	3）数值特征
		>排序  
		>找相邻点的中点作为分割点
			可以仅对两边类别不一致的分割点进行计算

--- 树的形成

选取基尼指数小的特征进行划分，会发现该特征的某个属性子集中，
其中一个目标值所占比例1，其他目标值所占比例为0，
则表示该属性子集很纯；
该特征的其他属性子集，根据已经有的属性子集样本，再按照其他特征的属性进行重新划分，
也能选取一个最优特征，也会发现某个属性子集纯了；
，，，
以此类推,
样本数每次都会减少，属性子集样本数每次也会减少，最后肯定会出现，都纯了；
数据集每次都能得到最优特征即子节点，每次都能划出一个属性即叶子节点，随着越来越纯，子节点和叶子节点就都确定了；就形成了树；

while(当前节点"不纯")：
    1.遍历每个变量的每一种分割方式，找到最好的分割点
    2.分割成两个节点N1和N2
end while
每个节点足够“纯”为止

但是，
一般不会让每一个属性即叶子结点都纯，
因为会导致二叉树过于复杂，
就会过拟合；

-- cart剪枝

树越繁茂，模型越复杂，模型就会过拟合；

1）图形描述
    横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。
    实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。
    随着树的增长，在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。
2）出现这种情况的原因：
    原因1：噪声、样本冲突，即错误的样本数据。
    原因2：特征即属性不能完全作为分类标准。
    原因3：巧合的规律性，数据量不够大。

为了防止过拟合，
需要剪枝。
决策树是一个强的非线性模型；也是一个最容易过拟合的模型；


-- 常用的减枝方法
2.1 预剪枝：设置条件，限制树的生长；
（1）指定每一个结点所包含的最小样本数目，
例如10，则该结点总样本数小于10时，则不再分；
（2）指定树的高度或者深度，
例如树的最大深度为4；
（3）指定结点的熵最小值；
小于某个值，不再划分。
随着树的增长，在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。
2.2 后剪枝：先生长，再剪枝
后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。

后剪枝的效果，比预剪枝要好；
但是后剪枝计算量要比预剪枝要大；

-- 特征工程-特征提取

将不能进行数值计算的特征，转换成能进行数值计算的特征；
将任意数据（如文本或图像）转换为可用于机器学习的数值特征

-- 特征提取分类:
    1）字典特征提取(特征离散化)；本质是one-hot提取；
    2）文本特征提取
    3）图像特征提取（深度学习将介绍）
    
-- 特征提取API
>> sklearn.feature_extraction

-- 字典特征提取:实现类别特征的one-hot映射；
	作用：对字典数据进行特征值化

>> from sklearn.feature_extraction import DictVectorizer
   transfer = DictVectorizer(sparse = False)
>> res = transfer.fit_transform(X)
    X:是一个列表，元素为字典；
    res：sparse矩阵,也就是稀疏矩阵，返回 含有元素下标的非零元素的部分稀疏矩阵；
    sparse：默认是True;设置为False,返回完整的稀疏矩阵；
    
>> transfer.get_feature_names() 
	返回类别名称；
	是一个列表，元素为字符串；
>> pd.DateFrame(res,columns = transfer.get_feature_names())
   将稀疏矩阵转换成二维数组

在矩阵中，若数值为0的元素数目远远多于非0元素的数目，并且非0元素分布没有规律时，则称该矩阵为稀疏矩阵；
与之相反，若非0元素数目占大多数时，则称该矩阵为稠密矩阵。 
定义非零元素的总数比上矩阵所有元素的总数为矩阵的稠密度。

对于特征当中存在类别信息的我们都会做one-hot编码处理



-- 文本特征提取
作用：对文本数据进行特征值化

-- 词频特征提取
>> from sklearn.feature_extraction.text import CountVectorizer
   vector = CountVectorizer()
   
>> res = vector.fit_transform(X)
    X:文本或者包含文本字符串的可迭代对象，语料库；
    返回值:返回sparse矩阵
    当语料库非常庞大，返回的是极度稀疏的词频矩阵
>> res.toarray()
	将稀疏矩阵转换成数组，返回词频向量；
>> vector.get_feature_names() 
	返回值:单词列表
>> vector.vocabulary()
	返回词典，语料库中出现的所有词，词典有多大，向量就有多长
	
>> pd.DataFrame(res.toarray(),columns= vector.get_feature_names())
   将数组转换成二维数组
   返回词频向量
   
   
-- 中英文都需要先分词，都不会纪录单个字的词
英文默认是以空格分开的，其实就达到了一个分词的效果；工业场景下，也需要对英文中多个词表达的是 一个词 进行分词；
中文默认是以标点符号分开的，所以更迫切需要分词；

-- jieba分词处理
需要安装下jieba库:
帮助分词，关键词提取，词性标注，文本摘要；
>> pip3 install jieba

>> 两种分词结果：
	1）返回迭代器：
	>> jieba.cut(x)
	>> for i in jieba.cut(x):
    		print(i)
    
    x:输入的文本
    i:遍历的词
    
	需要用循环遍历；
	当文本很长的时候，为了避免占用大量内存；
	2）返回分好之后的列表
	>> jieba.lcut(x)
	x:输入的文本
	
	当文本很长的时候，就会占用大量内存；
>> 三种分词模式：
	1）精确模式；
	默认情况下，是精确模式；
	精确模式就是在全模式下，选用 hmm 隐马尔科夫算法，选取概率比较大的词；
	>> '/'.join(jieba.lcut(text))
	将列表内的字符串，用‘/’进行拼接；
	返回一个字符串；
	
	2）全模式；
	>> '/'.join(jieba.lcut(text,cut_all = True))
	将文本又进一步的分词，列出所有可分的词，更加细微；
	返回一个字符串；
	
	3）搜索引擎模式；
	>> '/'.join(jieba.lcut_for_search(text))
	对精确模式下的分词长度 >= 3 的词，再次进行分词；
	返回一个字符串；
	
-- 给定分词器和自定义停用词

text = ['一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。','我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。','如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。']

stop_words = {'。','，','它','的','你','我们','还是'}

方式一：
	先分词，再特征提取；
	
方式二：
	直接进行特征提取
	只需要给定分词器和自定义停用词；
	
推荐使用方式二：
大大简化代码流程

>> cv = CountVectorizer(tokenizer=jieba.lcut，stop_words=stop_words)
    tokenizer:
        接收一个函数；
        tokenizer=jieba.lcut表明函数是做分词的；
    stop_words：
    	对于文本分类没有意义的词；
    	接收一个集合；
    	stop_words=stop_words表明分词就会忽略集合内的词；
    	
>> text1 = cv.fit_transform(text)
	text:原文本
>> text1.toarray()
	转换成数组
>> pd.DataFrame(text1.toarray(),columns=cv.get_feature_names())
	得到二维数组

-- 如何处理某个词或短语在多篇文章中出现的次数高这种情况

-- 词频特征的缺点：
	会放大常用词的重要性
	
-- Tf-idf文本特征提取	

1)词频（term frequency，tf）：
指的是某一个给定的词语在该文件中出现的频率；
假设，某文件共有词 m 个，其中某个词 有n 个，tf = n/m

2)逆向文档频率（inverse document frequency，idf）：
是一个词语普遍重要性的度量。
某一特定词语的idf，可以由 总文件数目 除以 包含该词语之文件的数目，再将 得到的商 取以10为底的 对数 得到
假设，总文件数目 有 i 个，包含某个词的文件数目 有 j 个，idf = log(i/j)

为了防止为 0 ，平滑变换后：
idf = log(i + 1)/(j + 1) + 1

则：
ifdif = tf * idf 
      = n/m * log(i/j)
tf 很大，同时 idf 就很小，从而if*idf相互抵消，值就很小；
ifdif 可以理解为重要程度。     
对分类有帮助的词，ifdif 就会很大；对分类没有帮助的词，ifidf 就会很小；

tfidf(i) = tf(i,j) * idf(i)
i:词下标
j:文章下标

常用词：词频很高，idf 很低，结果很低；
专用词：词频高，idf 也高，结果很高



-- TF-IDF的主要思想是：

如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
-- TF-IDF作用：
用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。

Tf-idf的重要性
分类机器学习算法进行文章分类中前期数据处理方式

2）tfidf特征提取   
>> from sklearn.feature_extraction.text import TfidfVectorizer

data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
            
stop_words = ['一种', '不会', '不要']

>> cv = TfidfVectorizer(tokenizer=jieba.lcut,stop_words=stop_words)
>> res = cv.fit_transform(data)
>> res.toarray()
词频向量都是小数，是因为做了正则化，是单位向量
>> cv.get_feature_names()
>> pd.DataFrame(res.toarray(),columns=cv.get_feature_names())


-- 决策树算法api
这个算法 api 是要求强制二叉树的；用来做分类的
>> from sklearn.tree import DecisionTreeClassifier
(from sklearn.tree import DecisionTreeRegressor  这个算法 api 是用来做回归的)
>>  dt = DecisionTreeClassifier()
(criterion=’gini’, max_depth=None,random_state=None)
    1）criterion：特征选择标准
    	用来计算不纯度的指标；
    	做分类："gini"或者"entropy"，前者代表基尼系数，后者代表信息增益。
    	做回归：mse 或者 mae
    	无论选择哪一个，都是cart树；
    	一默认"gini"，即CART算法。
    2）min_samples_split：内部节点再划分所需最小样本数
    值越大，限制越厉害；
    假设为 n ,即含有若干特征的样本集的数量或者某特征的某属性子集的样本数量，小于这个数就不能选择最优特征来进行划分；
    	
        这个值限制了子树继续划分的条件，
        如果某节点的样本数少于min_samples_split，
        则不会继续再尝试选择最优特征来进行划分。 
        默认是2.如果样本量不大，不需要管这个值。
        如果样本量数量级非常大，则推荐增大这个值。
        我之前的一个项目例子，有大概10万样本，建立决策树时，
        我选择了min_samples_split=10。可以作为参考。
    3）min_samples_leaf： 叶子节点最少样本数
    	 值越大，限制越厉害；
    	
        这个值限制了叶子节点最少的样本数，
        如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。
        默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。
        如果样本量不大，不需要管这个值。
        如果样本量数量级非常大，则推荐增大这个值。
        之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。
    4）max_depth：决策树最大深度
    	 值越小，限制越厉害；
    
        决策树的最大深度，默认可以不输入，
        如果不输入的话，决策树在建立子树的时候不会限制子树的深度。
        一般来说，数据少或者特征少的时候可以不管这个值。
        如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。
        常用的可以取值10-100之间
    5）random_state： 随机数种子
   

```

