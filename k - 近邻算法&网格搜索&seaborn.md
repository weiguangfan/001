k - 近邻算法

```
-- K Nearest Neighbor算法又叫KNN算法
-- 定义
如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。

-- KNN算法流程总结
1）计算已知类别数据集中的点与当前点之间的距离
2）按距离递增次序排序
3）选取与当前点距离最小的k个点
4）统计前k个点所在的类别出现的频率
5）返回前k个点出现频率最高的类别作为当前点的预测分类

-- 机器学习流程
1.获取数据集
2.数据基本处理
3.特征工程
4.机器学习
5.模型评估

-- Scikit-learn工具
-- 安装
>> pip3 install scikit-learn
-- K-近邻算法API
>> sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)
n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数
-- 代码过程
1 导入模块
>> from sklearn.neighbors import KNeighborsClassifier
2 构造数据集
x 必须是二维的，y 必须是一维的
>> x = [[0], [1], [2], [3]]
>> y = [0, 0, 1, 1]
3 机器学习 -- 模型训练
3.1 实例化API
# n_neighbors相当于knn中的k
>> knn=KNeighborsClassifier(n_neighbors=2)
3.2 使用fit方法进行训练
>> knn.fit(x, y)
3.3 使用训练好的模型进行预测
>> knn.predict([[1]])

-- K值选择说明
1,K值过小：
容易受到异常点的影响
2,k值过大：
受到样本均衡的问题

-- 过拟合和欠拟合
1,欠拟合：训练集和测试集的表现都不好。（模型过于简单）
通俗解释：测啥啥不准
定义：一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。
2,过拟合：训练集表现好，测试集表现不好。（模型过于复杂）
通俗解释：对训练的数据支持的非常非常好
定义：一个假设在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。

直观地理解，过拟合就是学习到了很多“局部信息”，或者是“噪音”，使得我们的模型中包含很多“不是规律的规律”。在knn算法中，k越小，就越有可能让我们的学习结果被“局部信息”所左右。在极端情况下，k=1，knn算法的结果只由离我们待预测样本最近的那个点决定，这使得我们knn的结果高概率被“有偏差的信息”或者“噪音”所左右，是一种过拟合。

随着模型复杂度x的增加，错误率y发生变化，呈现三个阶段，欠拟合&正常&过拟合；对应两条曲线，近似误差&估计误差；
模型复杂度x：可能是特征值，迭代次数，某一个超参数其中一个
欠拟合：随着x 的增大，y减小，近似误差和估计误差都减小；
过拟合：随着x 的增大，近似误差减少，估计误差增大；

比如：一个模型，特征值很少，就容易欠拟合；特征值过多，就容易过拟合；
我们的理想是取中间一段的图像，正常的那部分；


-- 过拟合原因以及解决办法
原因：模型过于复杂
	1）数据
		特征太多
	2）算法
      	模型参数太多
      	模型太强
      	模型迭代次数太多
      	正则化力度太小

原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点

解决办法：
	1）数据：
		1）减少特征
			1）特征选择
             2）特征降维
         2）增加数据量（特征多不多是相对于数据量来说的）
     2）算法：
     	1）减少参数
     	2）换一个简单的算法
      	3）早停
      	4）（早点停止训练）
        5）加大正则	
             
             
             
1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。
2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。
3）减少特征维度，防止维灾难
4) 对于Knn来说,近邻的数量非常的重要

-- 欠拟合原因以及解决办法
原因：模型过于简单
	1）数据
		特征太少，特征无用
	2）算法
		算法参数少
		算法弱
		迭代次数过少
		正则化力度太大
学习到数据的特征过少

解决办法：
	1）数据
		1）增加特征
	2）算法
		1）增加参数
		2）换一个强的算法
		3）多训练几轮
		4）减少正则化力度

1）添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。
2）添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强

-- 近似误差和估计误差
近似误差：可以理解为对现有训练集的训练误差。
估计误差：可以理解为对测试集的测试误差。

近似误差关注训练集，如果近似误差小了会出现过拟合的现象，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。模型本身不是最接近最佳模型。
估计误差：
可以理解为对测试集的测试误差，关注测试集，
估计误差小说明对未知数据的预测能力好，
模型本身最接近最佳模型。

-- 泛化能力：指模型对未知的、新鲜的数据的预测能力，通常是根据测试误差来衡量模型的泛化能力，测试误差越小，模型能力越强；

-- K值选择问题，李航博士的一书「统计学习方法」上所说：
1) 选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；
2) 选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。
3) K=N（N为训练样本个数），则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。
在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是把训练数据在分成两组:训练集和验证集）来选择最优的K值。

-- 问题导入：
实现k近邻算法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。
这在特征空间的维数大及训练数据容量大时尤其必要。
k近邻法最简单的实现是线性扫描（穷举搜索），即要计算输入实例与每一个训练实例的距离。计算并存储好以后，再查找K近邻。当训练集很大时，计算非常耗时。

-- kd树：为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树里，这样在计算之前从树里查询距离信息，尽量避免重新计算。其基本原理是，如果A和B距离很远，B和C距离很近，那么A和C的距离也很远。有了这个信息，就可以在合适的时候跳过距离远的点。

-- kd树的构建过程：模型训练把训练集训练成一棵kd树
1.首先选择一个轴进行划分
2.提取出该轴的中位数，将该轴坐标小于中位数的样本划到左子树；大于中位数的样本划到右子树
3.分别对左子树和右子树重复该动作（划分轴依次交替轮询），直到子树样本点只有一个为止。
需要关注细节：
a.选择向量的哪一维进行划分，首次选择0轴，后续交替轮询；
b.如何划分数据，选中位数，保证树是平衡二叉树（左子树和右子树差不多，减少搜索时间），

-- kd树的搜索过程：模型预测，来了一个样本，找它的最近邻
1.二叉树搜索比较待查询节点和分裂节点的分裂维的值，（小于等于就进入左子树分支，大于就进入右子树分支直到叶子结点）
2.顺着“搜索路径”找到最近邻的近似点
3.回溯搜索路径，并判断搜索路径上的结点的其他子结点空间中是否可能有距离查询点更近的数据点，如果有可能，则需要跳到其他子结点空间中去搜索
4.重复这个过程直到搜索路径为空

-- scikit-learn数据集API
-- sklearn.datasets
1，datasets.load_*()
获取小规模数据集，数据包含在datasets里
2，datasets.fetch_*(data_home=None)
获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录,默认是 ~/scikit_learn_data/

-- sklearn小数据集
>> sklearn.datasets.load_iris()
Iris数据集是常用的分类实验数据集，由Fisher, 1936收集整理。Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。

-- sklearn大数据集
>> sklearn.datasets.fetch_20newsgroups(data_home=None,subset=‘train’)
subset：'train'或者'test'，'all'，可选，选择要加载的数据集。
训练集的“训练”，测试集的“测试”，两者的“全部”

-- sklearn数据集返回值
load和fetch返回的数据类型datasets.base.Bunch(字典格式)
    data：特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组
    target：标签数组，是 n_samples 的一维 numpy.ndarray 数组
    DESCR：数据描述
    feature_names：特征名,新闻数据，手写数字、回归数据集没有
    target_names：标签名

-- 查看数据分布
-- seaborn介绍
Seaborn 是基于 Matplotlib 核心库进行了更高级的 API 封装，可以让你轻松地画出更漂亮的图形。而 Seaborn 的漂亮主要体现在配色更加舒服、以及图形元素的样式更加细腻。
>> 安装 
pip3 install seaborn
>> import seaborn as sns
>> sns.lmplot() 是一个非常有用的方法，它会在绘制二维散点图时，自动完成回归拟合
    sns.lmplot() 里的 x, y 分别代表横纵坐标的列名,
    data= 是关联到数据集,
    hue=*代表按照 species即花的类别分类显示,
    fit_reg=是否进行线性拟合。
    
demo1:
使用条形图显示每个分类栏中的观察计数。计数图可以被看作是一个横跨类别的直方图.
>> sns.countplot(x='Survived',data=train)
>> plt.xticks(np.arange(2),['derowed','survived'])
>> plt.title('Overall survived(training dataset)',fontsize= 18)
>> plt.xlabel('Passenger status after the tragedy',fontsize= 15)
>> plt.ylabel('Number of passengers',fontsize= 15)
>> labels = (train['Survived'].value_counts())
>> for i,v in enumerate(labels):
      plt.text(i,v-40,str(v),horizontalalignment = 'center',size = 14,color = 'w',fontweight = 'bold')
>> plt.show()
plt.text()给图形添加数据标签：
首先，前边设置的i、v-40值其实就代表了不同柱子在图形中的位置（坐标），通过for循环找到每一个i、v值的相应坐标——i、v，再使用plt.text在对应位置添文字说明来生成相应的数字标签，而for循环也保证了每一个柱子都有标签。其中，i, v-40表示在每一柱子对应i值、v值下方40处标注文字说明，horizontalalignment='center',（水平对齐）、verticalalignment（垂直对齐）的方式。

demo2:
将点估计和置信区间显示为矩形条。
条形图表示对每个矩形高度的数值变量的中心趋势的估计，并使用误差条提供关于该估计的不确定性的一些指示。
>> sns.barplot(x='Sex',y='Survived',data=train)
>> plt.title('Survived/Non-Survived Passenger Gender Distribution ',fontsize=10)
>> labels=['Female','Male']
>> plt.ylabel('%  of passenger survived',fontsize=8)
>> plt.xlabel('Gender',fontsize=8)
>> plt.show()
关于图像的解释：Seaborn会对'sex'列中的数值进行归类后按照estimator参数的方法（默认为平均值）计算相应的值，计算出来的值就作为条形图所显示的值（条形图上的误差棒则表示各类的数值相对于条形图所显示的值的误差）
hue（str）：dataframe的列名，按照列名中的值分类形成分类的条形图


demo3:
此函数提供对多个轴级函数的访问，这些函数使用多个可视化表示之一,显示数值变量与一个或多个分类变量之间的关系。“kind”参数选择要使用的基础轴级别函数：
>> sns.catplot(x='Sex',col='Survived',kind='count',data=train)
>> plt.show()


>> %matplotlib inline  
# 内嵌绘图
>> import seaborn as sns
>> import matplotlib.pyplot as plt
>> import pandas as pd
# 把数据转换成dataframe的格式
>> iris_d = pd.DataFrame(iris['data'], columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])
>> iris_d['Species'] = iris.target
>> def plot_iris(iris, col1, col2):
        sns.lmplot(x = col1, y = col2, data = iris, hue = "Species", fit_reg = False)
        plt.xlabel(col1)
        plt.ylabel(col2)
        plt.title('鸢尾花种类分布图')
        plt.show()
>> plot_iris(iris_d, 'Petal_Width', 'Sepal_Length')

-- 数据集的划分
机器学习一般的数据集会划分为两个部分：
1,训练数据：用于训练，构建模型
2,测试数据：在模型检验时使用，用于评估模型是否有效
-- 数据集划分api
>> sklearn.model_selection.train_test_split(arrays, *options)
    参数：
    x:数据集的特征值
    y:数据集的标签值
    test_size:测试集的大小，一般为float,可以把数据分割成指定比例的两部分
    random_state:不指定，每次分割结果会不一样，因为都会选择不同的随机数种子。指定，每次分割结果固定。随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。
    return:
    x_train, x_test, y_train, y_test

-- 特征预处理
通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程

-- 为什么我们要进行归一化/标准化？
特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响（支配）目标结果，使得一些算法无法学习到其它的特征.
我们需要用到一些方法进行无量纲化，使不同规格的数据转换到同一规格

-- 包含内容(数值型数据的无量纲化)
1,归一化
2,标准化

-- 特征预处理API
sklearn.preprocessing

-- 归一化
通过对原始数据进行变换把数据映射到(默认为[0,1])之间
最大值与最小值非常容易受异常点影响，即便做了归一化，数据还是会受到数值大小的影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。
>> sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)… )
方式一：
>> MinMaxScalar.fit(X)
计算出每一列的最大值max和最小值min，保存到对象的属性中，为将来做变换使用
>> MinMaxScalar.transform(X)
利用fit中计算的min,max进行转换
方式二：
>> MinMaxScalar.fit_transform(X)
计算的同时进行转换

X:numpy array格式的数据[n_samples,n_features]，必须接受二维数据
返回值：转换后的形状相同的array

-- 标准化
通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内
在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。
对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变
对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。
-- API
>> sklearn.preprocessing.StandardScaler( )
处理之后每列来说所有数据都聚集在均值0附近标准差为1
方式一：
>> StandardScaler.fit(X)
训练的时候，就计算每一列的均值mean和标准差std，并保存到属性中
>> StandardScaler.transform(X)
利用fit中计算的mean,std进行转换
方式二：
>> StandardScaler.fit_transform(X)
计算的同时进行转换

X:numpy array格式的数据[n_samples,n_features]，必须接受二维数据
返回值：转换后的形状相同的array

-- 特征工程：
标准化
-- 对训练集做了什么，也要对测试集同样做什么
-- 测试集要用训练集的参数进行转换
>> transfer = StandardScaler()
实例化api
>> x_train = transfer.fit_transform(x_train)
利用训练集进行训练转换
求解出训练集的均值和标准差
>> x_test = transfer.transform(x_test)
此时不用再fit
利用训练好的标准化模型对测试集进行转换
用训练集的均值和方差对测试集进行转换，而不能再去求解测试集的均值和方差

knn算法总结：
优点：
1，简单有效 原理简单 效果好（非线性分类器）
2，重新训练代价低 训练得时候不干事
3，适合类域交叉样本 （非线性分类器）
4，适合的样本自动分类（样本越多，越稳定）
缺点：
1，预测耗时 训练的时候最多构建kd 树，预测的时候需要预测距离，找出最近邻，计算量大
2，概率是基于统计频率得来
3，可解释性不强 抛弃了原特征，无法解释原特征 基于实例的算法
   1）knn 用近邻法做升维 原特征--近邻法--高维空间
   2）svm 用核方法做升维
   3）你是好人还是坏人，跟你是什么样的人没关系，跟你的朋友有关系
4，计算量大
```

```
-- 交叉验证
-- 交叉验证目的：为了让被评估的模型更加准确可信，为了更可靠的评估模型的好坏
注意：交叉验证并不能提高模型的准确率
将拿到的训练数据，分为训练和验证集。
将数据分成4份，其中一份作为验证集。然后经过4次(组)的测试，每次都更换不同的验证集。即得到4组模型的结果，取平均值作为最终结果。又称4折交叉验证。
-- 之前把数据分为训练集和测试集，但是为了让从训练得到模型结果更加准确。做以下处理
训练集：训练集+验证集 将训练集等分成k 份
测试集：测试集

-- 网格搜索
-- 网格搜索就是把这些超参数的值,通过字典的形式传递进去,然后进行选择最优值
通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数。
但是手动过程繁杂，所以需要对模型预设几种超参数组合。
每组超参数都采用交叉验证来进行评估。
最后选出最优参数组合建立模型。

-- 交叉验证，网格搜索（模型选择与调优）API：
>> from sklearn.model_selection import GridSearchCV
>> gs = GridSearchCV()
(estimator, param_grid=None,cv=None)
    对估计器的指定参数值进行详尽搜索
    1）estimator：估计器对象
    2) param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}
    3) cv：指定几折交叉验证
    4) n_jobs=-1:并行化运算
>> gs.fit(x_train,y_trian)：输入训练数据
>> gs.score(x_train,y_train)：准确率

    结果分析：
        bestscore__:在交叉验证中验证的最好结果
        bestestimator：最好的参数模型
        cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果

```

```

```

```

```

```

```

```

```

```

```

```

```

```

```

```

```