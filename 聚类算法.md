# 聚类算法#

```
--聚类算法：簇
	人以类聚，物以群分；
    一种典型的无监督学习算法，
    主要用于将相似的样本自动归到一个类别中。

    在聚类算法中根据样本之间的相似性，
    将样本划分到不同的类别中，
    对于不同的相似度计算方法，
    会得到不同的聚类结果，
    常用的相似度计算方法有 欧式距离法。

--聚类算法与分类算法最大的区别
    聚类算法是无监督的学习算法，
    而分类算法属于监督的学习算法。
	聚类的类是未知的，没有标签，不可解释；
	分类的类是已知的，有标签，可解释；
--聚类算法api
>>  from sklearn.cluster import KMeans
>>	km = KMeans()
	(n_clusters=8)
    参数:
        n_clusters:开始的聚类中心数量,簇数；
        整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。
    方法:
        estimator.fit(x),无监督，没有y;
        estimator.predict(x)
        estimator.fit_predict(x)
    		计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)
    		
>> from sklearn.datasets.samples_generator import make_blobs
>> X,y = make_blobs(n_samples=1000,n_features=2,centers=[[-1, -1],[0,0],[1,1],[2,2]],cluster_std=[0.4,0.2,0.2,0.2],random_state=9)
	sklearn中生成数据的方法，X是ndarray,
	n_samples:样本点数量；
	n_features：每个样本的特征值；
	centers：每个簇的中心点；
	cluster_std：标准差，值越大，越分散；值越小，越聚集；
	
--聚类算法实现流程
    --k-means其实包含两层内容：
        K : 簇数,每一个簇都有一个中心点
        means：中心点是通过计算每个簇所有样本平均值得到的；
    --k-means聚类步骤--EM算法阉割版，完整的EM算法是GMM高斯混合聚类
    自然语言处理学习路经：
    EM--HMM(隐马尔科夫)--CRF（条件随机场）
    EM算法思想是无监督学习的底层理论支撑；
        1、初始化k个中心点；
        2、聚簇； E
        	所有样本离谁近就给谁，分成k簇；
        3、更新中心点；M
        	计算每一簇样本的平均值
        4、不断重复2-3步，直到满足条件；
        	迭代步数；
        	中心点不再发生偏移；
        通过不断的迭代，不断地重新选定中心点和重新将样本分簇，最终中心点不发生改变；
--模型评估  
SSE、“肘”部法、SC系数和CH系数的实现原理
1.误差平方和(SSE \The sum of squares due to error)：
    1）示例：
        数据-0.2, 0.4, -0.8, 1.3, -0.7, 均为真实值和预测值的差
        SSE = (-0.2)**2+(0.4)**2+(-0.8)**2+(1.3)**2+(-0.7)**2=3.02
    2）在 k-means 中的应用:
        SSE = sum(i:1-k) sum( p -) C(i) ) |p-m(i)|**2
            p:样本点
            m(i)：簇的中心点
            C(i)：第i簇所有样本
            K：一共分为 k 个簇
        SSE图最终的结果,对图松散度的衡量；
        SSE随着聚类迭代,其值会越来越小,直到最后趋于稳定
    	如果质心的初始值选择不好,SSE只会达到一个不怎么好的局部最优解
    k-means存在一个问题：
    中心点的选择会影响聚类的结果；
    
2.“肘”方法 (Elbow method) — K值确定
    （1）对于n个点的数据集，迭代计算k from 1 to n，每次聚类完成后计算每个点到其所属的簇中心的距离的平方和；
    （2）平方和是会逐渐变小的，直到k==n时平方和为0，因为每个点都是它所在的簇中心本身。
    （3）在这个平方和变化过程中，会出现一个拐点也即“肘”点，下降率突然变缓时即认为是最佳的k值。
    在决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在增加分类无法带来更多回报时，我们停止增加类别。
    >> sse = []
    >> for k in range(1,11):
            km = KMeans(n_clusters=k)
            km.fit(X)
            sse.append(km.inertia_)
    >> plt.plot(range(1,11),sse)
    >> plt.show()
    选择肘部拐点作为最优k值点
    
3.轮廓系数法（Silhouette Coefficient）
结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果：
    
    S = (b-a)/max(a,b)
    a:avg，样本i到同一簇内其他点不相似程度的平均值
    b:min，样本i到其他簇的平均不相似程度的最小值
    S -) [-1,1]
    
    目的：内部距离最小化，外部距离最大化
    S(i) = (b(i)-a(i))/max{a(i),b(i)}
    
          1-a(i)/b(i)  a(i)<b(i)
    S(i)= 0            a(i)=b(i)
          b(i)/a(i)-1  a(i)>b(i)
         
          
    计算样本i到同簇其他样本的平均距离ai，ai 越小样本i的簇内不相似度越小，说明样本i越应该被聚类到该簇。
    计算样本i到最近簇Cj 的所有样本的平均距离bij，称样本i与最近簇Cj 的不相似度，定义为样本i的簇间不相似度：bi =min{bi1, bi2, ..., bik}，bi越大，说明样本i越不属于其他簇。
    
    1.求出所有样本的轮廓系数后再求平均值就得到了平均轮廓系数。
    2.平均轮廓系数的取值范围为[-1,1]，系数越大，聚类效果越好。
    3.簇内样本的距离越近，簇间样本距离越远
    
    每次聚类后，每个样本都会得到一个轮廓系数，当它为1时，说明这个点与周围簇距离较远，结果非常好，当它为0，说明这个点可能处在两个簇的边界上，当值为负时，暗含该点可能被误分了。
    
4.CH系数（Calinski-Harabasz Index）
    类别内部数据的协方差越小越好，类别之间的协方差越大越好（换句话说：类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好），
    这样的Calinski-Harabasz分数s会高，分数s高则聚类效果越好。
    
    S(k) =tr(B(k))/tr(W(k)) * (m-k)/(k-1)
    
    m为训练集样本数，k为类别数。
    tr为矩阵的迹,
    B(k)：簇间方差，越大越好
    W(k)：簇内方差，越小越好;
	.k越大，tr(B(k))/tr(W(k))就越大；但是极端情况下，k=m,w(k)=0,tr(B(k))/tr(W(k))就趋近于无穷大；
	为了抵消k的影响，系数(m-k)/(k-1)；
	.CH需要达到的目的：
    用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果。
    .ch值越大越好；
    
	使用矩阵的迹进行求解的理解：
	a11  a12 ,,, a1n
	a21  a22 ,,, a2n
	,,,
	an1  an2 ,,, ann
	
	迹/方差 = a11 + a22 + a33 ,,, + ann
	
	
    矩阵的对角线可以表示一个物体的相似性
    在机器学习里，主要为了获取数据的特征值，那么就是说，在任何一个矩阵计算出来之后，都可以简单化，只要获取矩阵的迹，就可以表示这一块数据的最重要的特征了，这样就可以把很多无关紧要的数据删除掉，达到简化数据，提高处理速度。
    
    
--算法优化
--k-means算法的优缺点
    优点：
        1.原理简单，实现容易
        2.效果不错
        3.样本数不多的情况下，计算量小
        算法复杂度o(IKMN):
            I:迭代次数
            k:中心点个数
            M:样本数
            N:特征数	

    缺点：
        1.容易受异常值影响，异常值会使均值点发生偏移
        2.不能够直观的对聚类结果进行观察
        3.结果是局部最优（em算法本身的原因）
        4.中心点的选择会影响聚类结果
--canopy、K-means++、二分K-means、K-medoids的优化原理
--Canopy算法配合初始聚类
    随机选中一个中心点 p,以 p 为圆心，T1 为半径，画圆 s1，圆内会有很多样本点(k1,k2,k3)；
    以 p 为圆心，T2 为半径，T2 > T1 ,画圆 s2，圆s2内，圆s1外 会有很多样本点(k5,k6,k7)；
    此时选择圆s2外一个样本点（k8,k9,k10,k11）,以k8 为圆心，T1 为半径，画圆 s3，s3和s2相交，和s1不相交，k6在两个圆的相交区域，T2 > 2T1;
    以k8 为圆心，T2 为半径,画圆 s4,s4与s1相交，k3在两个圆的相交区域；又包含了s3 外的k9;
    再选择k10为中心点，T1为半径，
    T2为半径，
    直到把所有样本点都包含在圆内；

    .小圆的圆心就是中心点，小圆形成一个个簇；
    .因为圆心之间的距离很远，决定了中心点不会很近；
    .又因为各个大圆内的样本点数目不一样，就可以解释样本在各个簇的分布情况；
    .可以忽略样本点比较少的簇，即异常点，只看其他簇中样本点的分布情况；
    
	综上，从而确定初始中心点，确定k值；

业界先用 canopy 跑一下样本，确定k值，然后输送给 kmeans

	canopy原理：
		1.T1,T2，T2>T1
		2.不断的画大圆和小圆
		3.作用：粗聚类，得到中心点和K值，喂给kmeans
    优点：
        1.Kmeans对噪声抗干扰较弱，通过Canopy对比，将较小的NumPoint的Cluster直接去掉有利于抗干扰。
        2.Canopy选择出来的每个Canopy的centerPoint作为K会更精确。
        3.只是针对每个Canopy的内做Kmeans聚类，减少相似计算的数量。

    缺点：
    	1.算法中 T1、T2的确定问题 ，依旧可能落入局部最优解
--K-means++--kmeans中默认使用的就是K-means++
--一个帮助kmeans选择初始中心点的算法；

P= (D(x)**2) /( sum(x -) X ) (D(x)**2) ) 

D(x)**2：其中一个点到中心点的距离的平方；
sum(x -) X ) (D(x)**2)：所有样本点到该中心点的距离的平方和；
距离越大，概率越大；

随机选择一个样本点，作为中心点 p1，计算其他点到该点的距离;
选择距离 p1点 越远的点作为中心点的 概率 就越大；
假设选择 p2点，再次选择距离p1,p2都很远的点，作为下一个中心点；
假设选择 p3点，，，，

最终样本点的中心点相互之间的距离，都是合适的；


--二分k-means
--k-medoids（k-中心聚类算法）
--Kernel k-means






--kernel K-means、ISODATA、Mini-batch K-means的优化原理
```

