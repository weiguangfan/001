线性回归算法  操作指南

```
-- 线性回归：
做回归：房价预测、销量预测、分数预测
模型：y = ax + b 是线性的
	1）训练：已知大量的x和y,用优化算法求解a 和 b,确定了一个模型函数：y = ax + b
	2）预测：已经知道a 和 b,给定 x,利用y = ax + b 计算出 y 的值
	3）分类：
		一元线性回归 y = ax + b
		多元线性回归 y = a1x1 + a2x2 + a3x3 + …… 
-- 回归分类
    1）线性关系：
        线性回归
    2）非线性关系：
        1》非线性模型：knn,svm,决策树
        2》多项式扩展 + 线性回归（多项式回归）：
        	1》多项式扩展：x -> x,x^2,x^3,…… 将数据从低维映射到高维，低维中是非线性的，高维中就是线性的
        	2》再用线性回归解决        	
        
-- 定义与公式
线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。
特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归

公式：矩阵多项式表达式
h(x) = w1x1 + w2x2 + w3x3 + …… + b = W^T X + b 

           
    (w1,       (x1,
W =  w2,   X =  x2,
     w3,        x3,
     ……)        ……)
     
特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型。
	
线性回归当中主要有两种模型，一种是线性关系，另一种是非线性关系。

单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系。多于两个，就是超平面的关系。

如果是非线性关系，那么回归方程可以理解为：w1 x1 + w2 x2^2 + w3 x3^3 + ……

-- 线性回归API
>> sklearn.linear_model.LinearRegression()
   lr.coef_：回归系数 w
   lr.intercept_:回归截距 b
   
-- 机器学习模型架构
   1，模型：y = wx + b ;有无数个
   2，损失函数：关于w,b的函数，值越小，说明模型越优;J(W)= 1/2 sum(i-m) (yi-w^T xi)**2 
   3，优化求解的算法：通过训练集，损失函数找到 w*,b*,进而确定 y = w*x + b*函数模型；给定测试值，就能给出预测值


1，给定数据集 D = {(xi,yi)}i->1-m,有m个样本；
其中xi =(xi1,xi2,xi3,……，xid),每个样本有d个特征值,
yi->R(线性回归的输出空间是整个输出空间)

线性回归试图学得：
	f(xi)=w^T xi + b   ，使得f(xi) ~= yi
为了便于讨论，使 b = w0*x0,其中x0=1
此时，
	w =(w0,w1,w2,w3,……,wd)，xi =(1,xi,x2,x3,……,xd)
期望学得的函数为，
	f(xi)=w^T xi
	
2，预测值和真实值之间一定存在差异e,对于每个样本：
     yi = f(xi) + ei
  <=>yi = w^T xi + ei
假设ei是独立同分布的，并且服从高斯分布（正态分布），即：
	高斯密度函数
yi = w^T xi + ei代入高斯密度函数，
	代入后的式子
3，结合联合概率公式p(AB)=P(A)p(B)，
将上式连乘，得到在已知参数w和数据x的情况下，预测值为y的条件概率，得到样本的联合概率，即似然函数：
	L(W)=……
	
	要使L(W)最大化，即极大似然，
	引入似然函数，为了根据样本估计参数值；
	对似然函数进行对数变换，通过对数可以将乘法变成加法，简化运算；
	log(ab)=log(a)+ log(b)
	对数变化是一个单调变化，不影响单调性；
引入对数似然函数：
	……-1/2 sum(i-m) (yi-w^T xi)**2
	
	提取目标函数：
	J(W)= 1/2 sum(i-m) (yi-w^T xi)**2 
	关于 w 的二次函数，开口朝上的抛物线；
	即线性回归的损失函数；
	
4，要使对数似然函数取最大值，只需要目标函数取最小值。
损失函数最小，叫最小二乘损失，也叫最小均方差损失，
如何去求模型当中的W，使得损失最小？目的是找到最小损失对应的W值
提取目标函数：
	J(W)= 1/2 sum(i-m) (yi-w^T xi)**2 
	J(W)= 1/2 sum(i-m) (h(xi)-yi)**2
	J(W)=1/2 (Xw-y)**2
是关于 w 的二次函数，开口朝上的抛物线；
要求最小值，只需要对 w 求导，等于0；	
	

-- 优化算法
线性回归经常使用的两种优化算法
正规方程(最小二乘)
梯度下降法

-- 正规方程(最小二乘)
J(W)=1/2 (Xw-y)**2
对w 求导：
    1/2 * 2(Xw-y)X=0
    (Xw-y)XX^T=0 X^T
    (Xw-y)XX^T(XX^T)^-1=0 X^T (XX^T)^-1
    (Xw-y)=0
    Xw=y
    X^T Xw=X^T y
    (X^T X)^-1 X^T Xw=(X^T X)^-1 X^T y
    w = (X^T X)^-1 X^T y
得到：
	W = (X^T X)^-1 X^T y  
X为特征值矩阵，y为目标值矩阵。直接求到最好的结果
另一种写法：
	J(W)=(Xw-y)^T (Xw-y)
	……
	对w求导：
	……
	令式子等于零：
	W = (X^T X)^-1 X^T y
缺点：当特征过多，矩阵会非常庞大，矩阵的逆运算计算量很大，甚至于无法求解

-- 梯度下降法
1）一个变量时，梯度就是该变量的导数值，只有正负之分；梯度只有增大或者减小；
    导数为正，说明递增，要想让函数值减少，参数值就减少，跟梯度方向相反；
    导数为负，说明递减，要想让函数值减少，参数值就增大，跟梯度方向相反；
2）多个变量时，每个变量都有一个偏导数，多个导数值一起构成一个向量，向量的方向就是梯度的方向；

-- 单变量函数的梯度下降
我们假设有一个单变量的函数 :J(θ) = θ^2
函数的微分:J、(θ) = 2θ
初始化，起点为： θ0 = 1
学习率：α = 0.4
开始进行梯度下降的迭代计算过程:
θ0 =1
θ1 = θ0 - α*2θ0
   =1-0.4*2*1
   =0.2
代入J(θ) = θ^2，函数值减小
θ2 = θ1 - α*2θ1
   =0.2-0.4*2*0.2
   =0.04
代入J(θ) = θ^2，函数值减小
θ3 = θ2 - α*2θ2
   =0.04-0.4*2*0.04
   =0.008
代入J(θ) = θ^2，函数值减小
θ4 = θ3 - α*2θ3
   =0.008-0.4*2*0.008
   =0.0016
代入J(θ) = θ^2，函数值减小
	……
一直到指定的迭代次数，或者函数值不再减少，才会停止；
严格意义，找不到最优解，只能近似接近；

-- 多变量函数的梯度下降
我们假设有一个目标函数 ：:J(θ) = θ1^2 + θ2^2
现在要通过梯度下降法计算这个函数的最小值。
我们通过观察就能发现最小值其实就是 (0，0)点。
但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值! 
我们假设初始的起点为: θ0 = (1, 3)
初始的学习率为:α = 0.1
函数的梯度为:▽:J(θ) =< 2θ1 ,2θ2>
进行多次迭代:
θ0 = (1, 3)
θ1 = θ0 -α*▽J(θ)
   =(1, 3)-0.1*(2*1 ,2*3)
   =(0.8,2.4)
θ2 = θ1 -α*▽J(θ)
   =(0.8, 2.4)-0.1*(2*0.8 ,2*2.4)
   =(0.64,1.92)
…… 

梯度下降（Gradient Descent）公式：
沿着负梯度的方向修改参数的值
θi+1 = θi - α*▽J(θ)|θi
1）α:学习率或者步长
   注意：学习率只有一个，所有参数公用一个学习率
   如果出现损失函数值不减反增，说明学习率过大，要调小
   学习率过大，导致梯度爆炸；学习率过小，导致下降速度太慢；
2）梯度前加一个负号，就意味着朝着梯度相反的方向前进！
优点：每次可以选择一小部分数据进行计算，计算量小

-- 常见的梯度下降算法有：

全梯度下降算法(Full gradient descent）,
随机梯度下降算法（Stochastic gradient descent）,
小批量梯度下降算法（Mini-batch gradient descent）,
随机平均梯度下降算法（Stochastic average gradient descent）

它们都是为了正确地调节权重向量，通过为每个权重计算一个梯度，从而更新权值，使目标函数尽可能最小化。其差别在于样本的使用方式不同。

--全梯度下降算法（FG）
每次拿所有样本进行损失计算；
慢，稳；
需要所有样本进内存；
损失函数固定；梯度值固定，更新所有的参数值固定；
1)计算训练集所有样本误差，对其求和再取平均值作为目标函数，即损失函数。
2)对损失函数进行梯度下降时，加载的是所有样本，每次迭代都要对所有样本求偏导，并且要加载到内存；
全梯度下降法,速度会很慢,并且不能在线更新模型，即在运行的过程中，不能增加新的样本。

-- 随机梯度下降算法（SG）
每次随机拿一个样本进行损失计算并迭代；
快，不稳；
不需要所有样本进内存；
损失函数变化；梯度值不固定，更新所有的参数值不固定；
1）其每轮计算的目标函数，即损失函数，不再是全体样本误差，而仅是单个样本误差；
每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解；
每次计算一个样本，速度比较快，加载不占很大的内存；

-- 小批量梯度下降算法（mini-batch）
每次拿batch_size个样本计算损失函数；
一般稳，一般快；
损失函数变化，但是波动没有那么厉害；
1）每次从训练样本集上随机抽取一个小样本集，
2）在抽出来的小样本集上采用FG迭代更新权重。

-- 随机平均梯度下降算法（SAG）
在内存中为每一个样本保存一个历史梯度值(dx1,dx2,dx3,……),刚开始是0；
每次随机选一个样本，计算梯度，更新自己的梯度值；--快
然后求出所有样本的梯度的平均值，用这个梯度值更新所有参数；-- 稳
梯度值固定，更新所有的参数值固定；
1）在内存中为每一个样本都维护一个旧的梯度，
2）随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，
3）然后求得所有梯度的平均值，进而更新了参数。

-- 通过 正规方程（最小二乘法）优化API
-- sklearn.linear_model.LinearRegression(fit_intercept=True)
-- lr = LinearRegression()
    参数
    fit_intercept：是否计算偏置，就是要不要计算y = wx + b 中的b;默认False 不要；True,要；
    属性
    lr.coef_：回归系数，y = wx + b 中的 w;
    lr.intercept_：偏置,y = wx + b 中的 b;


-- 通过 随机梯度下降 优化API
-- sklearn.linear_model.SGDRegressor(loss="squared_loss", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)
-- sgd = SGDRegressor(max—iter=1000)	
    SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。
    参数：
    max—iter:最大迭代次数
    eta0：学习率
    loss:损失类型，loss=”squared_loss”: 普通最小二乘法
    fit_intercept：是否计算偏置，就是要不要计算y = wx + b 中的b;默认False 不要；True,要；
    learning_rate : 学习方式 string, optional
        学习率填充
        'constant': eta = eta0  常量，学习率默认为 eta0=0.01；
        'optimal': eta = 1.0 / (alpha * (t + t0)) [default]  指数衰减；动态衰减，学习率逐渐减小
        'invscaling': eta = eta0 / pow(t, power_t)  衰减；动态衰减，学习率逐渐减小
       		 power_t=0.25:存在父类当中
	对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。
    属性：
    sgd.coef_：回归系数
    sgd.intercept_：偏置
   
-- 回归性能评估
均方误差(Mean Squared Error)MSE)评价机制
>> sklearn.metrics.mean_squared_error(y_true, y_pred)
    均方误差回归损失
    y_true:真实值
    y_pred:预测值
    return:浮点数结果
    
    
-- 正则化    
在解决 回归过拟合 中，我们选择正则化。
在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化
注：调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果

-- 正则化类别
-- L2正则化
作用：可以使得其中一些W的值都很小，都接近于0，削弱某个特征的影响
优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象
Ridge回归

-- L1正则化
作用：可以使得其中一些W的值直接为0，删除这个特征的影响
LASSO回归

-- 岭回归
>> sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)
    具有l2正则化的线性回归
    alpha:正则化力度，也叫 λ
    λ取值：0~1 1~10
    solver:会根据数据自动选择优化方法
    sag:如果数据集、特征都比较大，选择该随机梯度下降优化
    normalize:数据是否进行标准化
    normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据
    Ridge.coef_:回归权重
    Ridge.intercept_:回归偏置
    
Ridge方法相当于SGDRegressor(penalty='l2', loss="squared_loss"),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)

>> sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)
    具有l2正则化的线性回归，可以进行交叉验证
    coef_:回归系数
正则化力度越大，权重系数会越小
正则化力度越小，权重系数会越大

-- 线性回归的改进-岭回归

J(θ) = 1/m sum(i:1~m) (θi^T xi - yi)^2 + α sum(i:1~n) θi ^2

α 越大，后一项作用越大，正则加的越厉害，模型就越不容易过拟合；
α 太大，正则化的太厉害，模型就会欠拟合；
α 越大，模型走向欠拟合；α 越小，模型就走向过拟合；

从岭迹图，可以看到，α 越大，参数趋于0，模型走向欠拟合；α 越小，参数趋于无穷大，模型就走向过拟合

-- API

1)实现 随机平均梯度下降
>> sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)
>> rg = Ridge(alpha= ,)
    具有l2正则化的线性回归,常用参数alpha
    alpha:正则化力度，也叫 λ
    λ取值：0~1 1~10
    
    solver:选择算法；
    会根据数据自动选择优化方法；
    sag:随机平均梯度下降；
    如果数据集、特征都比较大，选择该随机梯度下降优化；
    normalize:自带标准化，一般不用；
    数据是否进行标准化
    normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据
    Ridge.coef_:回归权重
    Ridge.intercept_:回归偏置
    
2)实现 随机梯度下降
>>Ridge方法相当于SGDRegressor(penalty='l2', loss="squared_loss"),
只不过SGDRegressor实现了一个普通的 随机梯度下降 学习，推荐使用Ridge(实现了SAG)

3)实现 交叉验证
>> sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)
>> rgcv = RigeCV(alphas=(,,,,,))
    具有l2正则化的线性回归，可以进行交叉验证
    选取最优参数，得到最合适的模型；
    coef_:回归系数

-- 模型的保存和加载
from sklearn.externals import joblib
    保存路径扩展名一定是 .pkl
    
    保存：
    joblib.dump(模型名, '文件名.pkl')
    比如：保存标准化模型，岭回归改进模型等
    
    加载：
    模型名 = joblib.load('路径')
    比如：加载模型后，就可以对数据进行标准化和预测
    
这样就很方便，训练好一个模型，保存起来；可以随时拿到其他文件中，加载后，直接就可以用；
```

