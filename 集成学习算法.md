#集成学习算法

```
集成学习通过建立几个模型来解决单一预测问题。
它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。
这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。
在打比赛时，成绩较好的队伍几乎都用了集成学习(ensemble learning)的方法。
集成学习的思想，简单来讲，就是“三个臭皮匠顶个诸葛亮”。
集成学习通过结合多个学习器(例如同种算法但是参数不同，或者不同算法)，一般会获得比任意单个学习器都要好的性能，尤其是在这些学习器都是"弱学习器"的时候提升效果会很明显。

--机器学习的两个核心任务
1. 任务一：如何优化训练数据 —> 主要用于解决欠拟合问题--boosting
2. 任务二：如何提升泛化性能 —> 主要用于解决过拟合问题--bagging

--集成学习中boosting和bagging
(bagging只是一个框架)
    欠拟合问题--弱弱组合变强（boosting逐步增强学习，基础模型都是弱模型）
    过拟合问题--互相遏制变壮（bagging采样学习集成，基础模型都是强模型）
    只要单分类器的表现不太差，集成学习的结果总是要好于单分类器的

--boosting和bagging区别
    1.数据差异化：
        bagging:使用有放回抽样
        boosting:使用修改样本权重
    2.训练过程：
        bagging:可以并行化训练
        boosting:只能串行
    3.模型融合：
        bagging:平权投票
        boosting:加权投票
    4.追求目标：
        bagging:追求低方差 稳
        boosting:追求低偏差 准
    5.子模型：
        bagging:强模型（过拟合模型）
        boosting:弱模型（欠拟合模型）


(1)--Bagging集成原理
    --采样（采集数据集，各个数据集有差异）
    --学习（模型训练，得到不同的模型，模型都是同一种类型）
    --集成（给定测试值，得到不同预测结果，数量最多的结果，作为最终结果，也就是所谓的融合）

--随机森林构造过程
    随机森林是一个算法；
    随机森林 约等于 Bagging + 决策树 + 列抽样

在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。
    --给定数据集
    --有放回抽样，随机抽取m个样本子集，每个样本子集随机选n个特征
    --训练成m个决策树
    --平权投票集成m个决策树；

    --两个随机：
        样本随机抽样--行随机--有放回抽样
        特征随机抽样--列随机

    --抽样的意义：
    加大数据集之间的差异化，从而使得模型差异化；

	--模型融合的基础是模型差异化
	
--随机森林api
>> from sklearn.ensemble import RandomForestClassifier
>> dfc = RandomForestClassifier()
(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)
    1）n_estimators：
    integer，optional（default = 10）森林里的树木数量，要训练多少棵树
    2）Criterion：
    string，可选（default =“gini”）分割特征的测量方法，选择基尼还是熵
    3）max_depth：
    integer或None，可选（默认=无）树的最大深度 5,8,15,25,30
    4）max_features="auto”,每个决策树的最大特征数量，列抽样时选取的特征数
    If "auto", then max_features=sqrt(n_features).原样本特征开根号个
    If "sqrt", then max_features=sqrt(n_features)(same as "auto").
    If "log2", then max_features=log2(n_features).原样本特征取对数
    If None, then max_features=n_features.
    5）bootstrap：boolean，optional（default = True）
    是否在构建树时使用放回抽样，当数据集很大，千万级别，可以选择不放回抽样；
    6）min_samples_split:节点划分最少样本数
    7）min_samples_leaf:叶子节点的最小样本数
    超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf
    
--bagging集成优点
    Bagging + 决策树/线性回归/逻辑回归/深度学习… = bagging集成学习方法
    


(2)--boosting集成原理
    随着学习的积累从弱到强；
    简而言之：每新加入一个弱学习器，整体能力就会得到提升；
    代表算法：Adaboost，GBDT，XGBoost
    Boosting系列算法里最著名算法主要有AdaBoost和提升树(Boosting tree)系列算法;
--实现过程：
    1>第一轮：
    对训练集m中样本点划分类别A/B，假设有一条分割线，左侧A右侧B；
    第一次将右侧B内，划错类别的A样本点，权重放大n倍，即复制了n份，从而改变样本分布,得到新的训练集m1；
    第二次对训练集m1中样本点划分类别A/B，假设有一条分割线，左侧A右侧B，得到模型M1；
    2>第二轮：
    第一次将左侧A内，划错类别的B样本点，权重放大n倍，即复制了n份，从而改变样本分布,得到新的训练集m2；
    第二次对训练集m2中样本点划分类别B/A，假设有一条分割线，左侧B右侧A，得到模型M2；
    3>第三轮：
    第一次将左侧B内，划错类别的A样本点，权重放大n倍，即复制了n份，从而改变样本分布,得到新的训练集m3；
    第二次对训练集m2中样本点划分类别A/B，假设有一条分割线，左侧A右侧B，得到模型M3；
    ，，，
    ，，，
    ，，，
    直到指定轮数或者将所有样本点分类正确为止

    Boosting指的是一类集成方法，其主要思想就是将弱的基学习器提升(boost)为强学习器。
    具体步骤如下:
    .先用每个样本权重相等的训练集训练一个初始的基学习器；
    .根据上轮得到的学习器对训练集的预测表现情况调整训练集中的样本权重(例如提高被错分类的样本的权重使之在下轮训练中得到更多的关注), 然后据此训练一个新的基学习器；
    .重复2直到得到[公式]个基学习器，最终的集成结果是[公式]个基学习器的组合。
    
    
    。不需要采样；
    。训练的时候，模型依次进行，只能串行；
    。预测的时候，模型已经出来，进行融合，可以并行；

--关键点：
    AdaBoost(Adaptive Boosting, 自适应增强)算法采取的方法是:
    .提高上一轮被错误分类的样本的权值，降低被正确分类的样本的权值；
    .线性加权求和。误差率小的基学习器拥有较大的权值，误差率大的基学习器拥有较小的权值。

1>如何确认投票权重？
    
	e(m):第m个学习器的错误率
	a(m) = 1/2 ln（(1-e(m)/e(m)）
    错误率=0.5，a(m)=0，没有投票权重；
    错误率>0.5，a(m)<0,投负权重票；
    错误率<0.5，a(m)>0,投正权重票；
    核心原则：正确率越高，投票权重越高；

2>如何调整数据分布？

                                         e^(-a(m))  预测值=真实值
    D(m+1) (x)   =  (D(m) (x))/Z(m)   *  e^(a(m)   预测值！=真实值
    Z(m)：归一化系数，将权重缩放到0-1

    --核心原则：
    前一轮学习正确的数据，忽略；
    前一轮学习错误的数据，重视；

3>AdaBoost构造过程：
	考虑如下形式的二分类（标准AdaBoost算法只适用于二分类任务）训练数据集:
	{(x1,y1),(x2,y2),,,(xN,yN)},其中xi是一个含有d个元素的列向量, 即
	xi -) X ;yi是标量，y-){-1,1}
	
    1.初始化训练数据，权重相等，假设有n个样本点，权重为1/n；
        D1=(W11,W12,,,W1N),
        W1i = 1/N,
        i=1,2,,,N
    2.对 m=1,2,,,M 重复以下操作得到 M 个基本学习器；
        (1) 按照样本权重分布 D(m) 训练数据得到第m个基学习器G(m) (x):
            G(m) (x):x-->{-1,1}
        (2) 计算Gm(x)在加权训练数据集上的分类误差率：
            e(m) = sum(i:1-N) P(G(m) (xi != yi))

    3.计算该学习器的权重；
        a(m) = 1/2 * ln( (1-e(m))/e(m) )

    4.更新训练样本权重
        D(m+1)=(W(m,1),W(m,2),,,W(m+1,N)
        W((m+1),i) = W(m,i)/Z(m)  *  exp(-a(m) y(i) G(m)(x(i))),i=1,2,,,N
        其中Z(m)是规范化因子，目的是为了使D(m+1)的所有元素和为1。即
        Z(m) = sum(i:1-N)  W(m,i) * exp(-a(m) y(i) G(m)(x(i))) 
        
                                             e^(-a(m))  预测值=真实值
        D(m+1) (x)   =  (D(m) (x))/Z(m)   *  e^(a(m))   预测值！=真实值
        
	5.构建最终的分类器线性组合:
        f(x) = sum(i:1-M) a(m) * G(m) (x)
        这是一个“加性模型(additive model)”。
            f(x)大于0；
            f(x)小于0；
            f(x)等于0；
        得到最终的分类器为:
        G（x）= sign(f(x)) = sign(sum(i:1-M)  a(m) * G(m) (x))
        G(X)大于0--> 1；
        G(X)小于0--> -1;
        最终实现分类的目的；
        D(m) (x)：第 m 轮样本权重
        D(m+1) (x)：第 m+1 轮样本权重
        a(m)：第m个模型权重；
        Z(m)：归一化系数，将权重缩放到0-1；
        
        符号函数（一般用sign(x)表示）是很有用的一类函数，能够帮助我们在几何画板中实现一些直接实现有困难的构造。 
        符号函数 能够把函数的符号析离出来 。
        在数学和计算机运算中，其功能是取某个数的符号（正或负）： 当x>0，sign(x)=1;当x=0，sign(x)=0; 当x<0， sign(x)=-1；
		

        .当基学习器 Gm(x) 的误差率 em<=0.5 时，am>=0，并且 am 随着 em 的减小而增大，即分类误差率越小的基学习器在最终集成时占比也越大。
        .即AdaBoost能够适应各个弱分类器的训练误差率，这也是它的名称中"适应性(Adaptive)"的由来。
        .被基学习器 Gm(x) 误分类的样本权值得以扩大，而被正确分类的样本的权值被得以缩小。
        .所有的 am 的和并不为1(因为没有做一个softmax操作)，f(x) 的符号决定了所预测的类，其绝对值代表了分类的确信度。

        --前向分步算法
        我们希望这个模型在训练集上的经验误差最小，即
            min sum(i:1-N) L(yi,f(x))  <---> min sum(i:1-N) L(yi,sum(i:1-M) am * Gm(x))	
        通常这是一个复杂的优化问题。
        前向分步算法求解这一优化问题的思想就是: 
        因为最终模型是一个加性模型，如果能从前往后，每一步只学习一个基学习器 Gm(x) 及其权重 am, 不断迭代得到最终的模型，那么就可以简化问题复杂度。
        具体的，当我们经过 m-1 轮迭代得到了最优模型fm-1 (x):
            fm(x)=fm-1 (x) + am * Gm(x)
        所以此轮优化目标就为
            min sum(i:1-N) L(yi,fm-1 (x) + am * Gm(x))

        求解上式即可得到第 m 个基分类器 Gm(x) 及其权重 am。
        这样，前向分步算法就通过不断迭代求得了从 m=1 到 m=M 的所有基分类器及其权重，问题得到了解决。

--boosting-AdaBoost-api
>> from sklearn.ensemble import AdaBoostClassifier
>> ada = AdaBoostClassifier()
(base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)
    1.base_estimatorobject, default=None
    2.n_estimatorsint, default=50
    树的数量，迭代的步数
    3.learning_ratefloat, default=1.
    学习率，步长
    4.algorithm{‘SAMME’, ‘SAMME.R’}, default=’SAMME.R’
    5.random_stateint or RandomState, default=None
    
(3)-boosting-GBDT
	GBDT原理理解：用梯度下降在函数空间优化求解最优函数
	
    梯度提升决策树(GBDT Gradient Boosting Decision Tree) 是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。
    它在被提出之初就被认为是泛化能力（generalization)较强的算法。
    >GB:梯度提升
        利用梯度下降算法优化损失函数，损失函数值减小，模型性能得到提升
    >DT:决策树一定是回归树
        不管GBDT解决的是回归问题还是分类问题，底层的树都是回归树
        为什么不用CART分类树呢？因为GBDT每次迭代要拟合的是梯度值，是连续值所以要用回归树。
        对于回归树算法来说最重要的是寻找最佳的划分点，那么回归树中的可划分点包含了所有特征的所有可取的值。
        在分类树中最佳划分点的判别标准是熵或者基尼系数，都是用纯度来衡量的，
        但是在回归树中的样本标签是连续数值，所以再使用熵之类的指标不再合适，取而代之的是平方误差，它能很好的评判拟合程度。
    >衰减（学习率learning_rate）：
        学习率*梯度 = 学习率*残差
        
        
    。损失函数：
    loss = f(w)
	
    。损失函数值逐次减小：
    w(1)=w(0) - α*▽f(w(0),x) 
    w(2)=w(1) - α*▽f(w(1),x)
    w(3)=w(2) - α*▽f(w(2),x)
    w(4)=w(3) - α*▽f(w(3),x)
    w(5)=w(4) - α*▽f(w(4),x)

    。以上式子两边对应相加：
    w(1)+w(2)+w(3)+w(4)+w(5)=w(0) - α*▽f(w(0),x)+w(1) - α*▽f(w(1),x)+w(2) - α*▽f(w(2),x)+w(3) - α*▽f(w(3),x)+w(4) - α*▽f(w(4),x)

    。变换后：
    w(5)=w(0)-α*▽f(w(0),x)-α*▽f(w(1),x)-α*▽f(w(2),x)-α*▽f(w(3),x)-α*▽f(w(4),x)

    α=1 hi(x)=▽f(w(j),x)  H(x)-boosting 集成表达方式
    H(X)=h(0)(x) + h(1)(x) + h(2)(x) + ,,,  

    如果上式中的 hi(x)=决策树模型,
    则上式就变为:
    GBDT = 梯度下降 + Boosting + 决策树
	
    --GBDT代码实现流程：
        不断地去拟合残差
	。计算损失函数,并求出第一个预测值:
        回归问题，损失函数为均方差损失，真实值-预测值
        loss = 1/2m sum(i:1-m) (y-`y)**2
        对`y求导：
            -1/m sum(i:1-m) (y-`y)
        导数为零：
            将样本点代入，解得y`,进而解得h(0)(x)，其实就是平均值；
	。求解划分点
		真实值-预测值=残差
        如果将所有样本点代入上式，得到包含三列：真实值、预测值、残差
        1.将 残差列，作为总样本的 目标值 列；GBDT 通过修改目标值实现数据的差异化；得到样本集M1；
        2.将M1训练成一棵树；以某特征的某属性进行划分，计算左右子树方差，再求和，得到该属性划分的方差；
        3.选取所有属性划分后，方差最小的哪一个特征属性，作为划分点；
        4.把每个子集的平均值作为预测值，求解得出h1(x)；
        5.对比第一轮的目标值和第二轮的预测值，相减，作为残差列 得到样本集M2；
        6.重复2-3步；
        7.最终得到结果：
        	 H(X)=h(0)(x) + h(1)(x) + h(2)(x) + ,,,  
        	 即得到最终的模型
        任意给定一个测试样本，通过该模型，就可以得到预测值；
        GBDT主要执行思想
        1.使用梯度下降法优化代价函数；
        2.使用回归树作为弱学习器，负梯度作为目标值；
        3.利用boosting思想进行集成。
（4）--XGBoost
    用牛顿法在函数空间优化求解最优函数
    XGBoost= 二阶泰勒展开+boosting+决策树+正则化
    Boosting：XGBoost使用Boosting提升思想对多个弱学习器进行迭代式学习
    二阶泰勒展开：每一轮学习中，XGBoost对损失函数进行二阶泰勒展开，使用一阶和二阶梯度进行优化。
    决策树：在每一轮学习中，XGBoost使用决策树算法作为弱学习进行优化。
    正则化：在优化过程中XGBoost为防止过拟合，在损失函数中加入惩罚项，限制决策树的叶子节点个数以及决策树叶子节点的值。
    --XGBoost 与 GBDT的区别：
    	.泰勒展开：
    		GBDT：一阶泰勒展开
        	XGBoost：二阶泰勒展开
        .优化方法：
            GBDT：梯度下降
            XGBoost：牛顿法
        .代码实现上XGBoost的改进：
        	添加正则项；防止过拟合
        		正则项=叶子节点个数+叶子节点分值的平方和（L2）
        	预排序；加快训练速度；
        	并行化
        		在框架上是不能并行的；xgboosting 也是boosting，必须训练好上一个模型，下一个模型才能才能训练；
```

